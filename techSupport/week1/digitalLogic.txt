Remember when I said that a computer is a device that stores and processes data by performing calculations? Whether you're creating an artificial intelligence that can beat humans at chess or something more simple, like running a video game, the more computing power you have access to, the more you can accomplish. By the end of this lesson, you'll understand what a computer calculates, and how. Let's look at this simple math problem. 0 +1 equals what? It only takes a moment to come up with the answer 1, but imagine that you needed to do 100 calculations that were this simple. You could do it, and if you were careful, you might not make any mistakes. Well, what if you needed to do 1,000 of these calculations? How about 1 million? How about 1 billion? This is exactly what a computer does. A computer simply compares 1s and 0s, but millions or billions of times per second. Wowza! The communication that a computer uses is referred to as binary system, also known as base-2 numeral system. This means that it only talks in 1s and 0s. You may be thinking, okay, my computer only talks in 1s and 0s. How do I communicate with it? Think of it like this. We use the letters of the alphabet to form words and we give those words meaning. We use them to create sentences, paragraphs, and whole stories. The same thing applies to binary, except instead of A, B, C, and so on, we only have 0 and 1 to create words that we give meaning to. In computing terms, we group binary into 8 numbers, or bits. Technically, a bit is a binary digit. Historically, we used 8 bits because in the early days of computing, hardware utilized the base-2 numeral system to move bits around. 2 to the 8th numbers offered us a large enough range of values to do the computing we needed. Back then, any number of bits was used, but eventually the grouping of 8 bits became the industry standard that we use today. You should know that a group of 8 bits is referred to as a byte. So a byte of zeroes and ones could look like 10011011. Each byte can store one character, and we can have 256 possible values, thanks to the base-2 system, 2 to the 8th. In computer talk, this byte could mean something like the letter C.
Play video starting at :2:22 and follow transcript2:22
And this is how computer language was born. Let's make a quick table to translate something a computer might see into something we'd be able to recognize. What does the following translate to?
Play video starting at :2:35 and follow transcript2:35
Did you get hello? Pretty cool, right?
Play video starting at :2:38 and follow transcript2:38
By using binary, we can have unlimited communication with our computer. Everything you see on your computer right now, whether it's a video, an image, text or anything else, is nothing more than a 1 or a 0. It is important you understand how binary works. It is the basis for everything else we'll do in this course, so make sure you understand the concept before moving on.

Remember from the earlier video that a byte can store only zeros and ones. That means we can have 256 possible values. By the end of this video, you'll learn how we can represent the words, numbers, emojis and more we see on our screens, from only these 256 possible values. It's all thanks to character encoding. Character encoding is used to assign our binary values to characters so that we as humans can read them. We definitely wouldn't want to see all the text in our emails and Web pages rendered in complex sequences of zeros and ones. This is where character encodings come in handy. You can think of character encoding as a dictionary. It's a way for your computers to look up which human characters should be represented by a given binary value. The oldest character encoding standard used this ASCII. It represents the English alphabet, digits, and punctuation marks. The first character in ASCII to binary table, a lowercase a, maps to 0 1 1 0 0 0 0 1 in binary. This is done for all the characters you can find in the English alphabet as well as numbers and some special symbols. The great thing with ASCII was that we only needed to use 127 values out of our possible 256. It lasted for a very long time, but eventually it wasn't enough. Other character encoding standards recreated to represent different languages, different amounts of characters and more. Eventually they would require more than 256 values we were allowed to have. Then came UTF 8. The most prevalent encoding standard used today. Along with having the same ASCII table, it also lets us use a variable number of bytes. What do I mean by that? Think of any emoji. It's not possible to make emojis with a single byte, so as we can only store one character in a byte, instead UTF 8 allows us to store a character in more than one byte, which means endless emoji fun. UTF 8 is built off the Unicode Standard. We won't go into much of detail, but the Unicode Standard helps us represent character encoding in a consistent manner. Now that we've been able to represent letters, numbers, punctuation marks and even emojis, how do we represent color? Well, there are all kinds of color models. For now, let's stick to a basic one that's used in a lot of computers. RGB or red, green, and blue model. Just like the actual colors, if you mix a combination of any of these, you'll be able to get the full range of colors. In computerland, we use 3 characters for the RGB model. Each character represents a shade of the color and that then changes the color of the pixel you see on your screen. With just eight combinations of zeros and ones, were able to represent everything that you see on your computer, from a simple letter a, to the very video that you're watching right now on the Coursera website. Very cool. In the next video, we'll discuss how we actually generate the zeros and ones.

> BINARY NUMBERS
You might be wondering how our computers get these ones and zeros. It's a great question. Imagine we have a light bulb and a switch that turns the state of the light on or off. If we turn the light on, we can denote that state is one. If the light bulb is off, we can represent the state is zero. Now imagine eight light bulbs and switches, that represents eight bits with a state of zero or one. Let's backtrack to the punched cards that were used in Jacquard's loom. Remember that the loom used cards with holes in them. When the loom would reach a hole it would hooked to thread underneath, meaning that the loom was on. If there wasn't a hole, it would not hook the thread, so it was off. This is a foundational binary concept. By utilizing the two states of on or off, Jacquard was able to weave intricate patterns of the fabric with his looms. Then the industry started refining the punch cards a little more. If there was a hole, the computer would read one. If there wasn't a hole, it would read zero. Then, by just translating the combination of zeros and ones, our computer could calculate any possible amount of numbers. Binary in today's computer isn't done by reading holes. It uses electricity via transistors allowing electrical signals to pass through. There's an electric voltage, we would denote it as one. If there isn't, we would denote it by zero. For just having transistors isn't enough for our computer to be able to do complex tasks. Imagine if you had two light switches on opposite ends of a room, each controlling a light in the room. What if when you went to turn on the light with one switch, the other switch wouldn't turn off? That would be a very poorly designed loom. Both switches should either turn the light on or off depending on the state of the light. Fortunately, we have something known as logic gates. Logic gates allow our transistors to do more complex tasks, like decide where to send electrical signals depending on logical conditions. There are lots of different types of logic gates, but we won't discuss them in detail here. If you're curious about the role that transistors and logic gates play in modern circuitry, you can read more about it in the supplementary reading. Now we know how our computer gets its ones and zeros to calculate into meaningful instructions. Later in this course, we'll be able to talk about how we're able to turn human-readable instructions into zeros and ones that are computer understands through a compilers. That's one of the very basic building blocks of programming that's led to the creation of our favorite social media sites, video games, and just about everything else. And I'm super excited to teach you how to count in binary, that's up next.

Binary is the fundamental communication block of computers, but it's used to represent more than just text and images. It's used in many aspects of computing like computer networking, which you'll learn about in a later course. It's important that you understand how computers count in binary. We've shown you simple lookup tables that you can use like the ASCII to binary table, but as an IT support specialist, whether you're working on networking or security, you'll need to know how binary works. So let's get started. You'll probably need a trusty pen and paper, a calculator, and some good old-fashioned brain power to help you in this video. The binary system is how our computers count using ones and zeros, but humans don't count like that. When you were a child, you may have counted using ten fingers on your hand. That innate counting system is called the decimal form or base-10 system. In the decimal system, there are 10 possible numbers you can use ranging from zero to nine. When we count binary, which only uses zero and one, we convert it to a system that we can understand, decimal. 330, 250, 2, 40, 4 million, they're all decimal numbers. We use the decimal system to help us figure out what bits our computer can use. We can represent any number in existence just by using bits. That's right. And we can represent this number just using ones and zeros. So how does that work? Let's consider these numbers: 128, 64, 32, 16, 8, 4, 2, and 1. What patterns do you see? Hopefully, you'll see that each number is a double of the previous number going right to left. What happens if you add them all up? You get 255. That's kind of weird. I thought we could have 256 values for a byte. Well, we do. The zero is counted as a value, so the maximum decimal number you can have is 255. What do you think the number is represented here? See where the ones and the zeros are represented. Remember, if our computer sees a one, then the value was on. If it sees a zero, then the value is off. If you add these numbers up, you'll get a decimal value. If you guessed 10, then you're right. Good job. If you didn't get it, that's okay too. Take another look. The 2 and 8 are on, and if we add them up, we get 10. Let's look at our ASCII to binary table again. The letter h in binary is 01101000. Now, let's look at an ASCII to decimal table. The letter h in decimal is 104. Now, let's try our conversion chart again. 64 plus 32 plus 8 equals 104. Look at that. The math checks out. Now, we're cooking. Wow! We've gone over all the essentials of the basic building blocks of computing and machine language. Next, you're going to learn how we build on top of this layer of computing to perform the task you'll do day to day.

